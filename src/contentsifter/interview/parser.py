"""Transcript parser for voice capture interviews.

Splits a voice transcript into Q&A pairs by matching questions from
the generated questionnaire, then stores each answer as a content_item.
"""

from __future__ import annotations

import difflib
import json
import logging
import re
from pathlib import Path

from contentsifter.storage.database import Database

log = logging.getLogger(__name__)

# Minimum answer length to keep (skip very short / "skip" responses)
MIN_ANSWER_CHARS = 30


def _enrich_questions_with_library(questions: list[dict]) -> None:
    """Cross-reference parsed questionnaire questions with the prompt library.

    Adds category, framework, and question_id fields to each question dict.
    Uses the question number to find the corresponding prompt in library order.
    """
    from contentsifter.interview.prompts import get_all_prompts

    library = get_all_prompts()

    # Build index: question_number -> prompt (1-based, matching Q1, Q2, etc.)
    lib_by_num = {i + 1: p for i, p in enumerate(library)}

    for q in questions:
        lib_prompt = lib_by_num.get(q["q_number"])
        if lib_prompt:
            q["question_id"] = lib_prompt["id"]
            q["category"] = lib_prompt["category"]
            q["framework"] = lib_prompt["framework"]
            q["content_type"] = lib_prompt["content_type"]


def parse_interview_transcript(
    transcript_path: Path,
    questionnaire_path: Path,
    db: Database,
    author: str = "",
) -> list[dict]:
    """Parse a voice transcript against a questionnaire and insert content items.

    Args:
        transcript_path: Path to the transcript text file.
        questionnaire_path: Path to the questionnaire markdown (generated by generator.py).
        db: Database connection.
        author: Client name for content items.

    Returns:
        List of inserted content item dicts.
    """
    transcript_text = transcript_path.read_text()
    questions = extract_questions_from_questionnaire(questionnaire_path)

    if not questions:
        log.warning("No questions extracted from questionnaire: %s", questionnaire_path)
        return []

    # Enrich questions with metadata from the prompt library
    _enrich_questions_with_library(questions)

    log.info("Matching %d questions against transcript (%d chars)", len(questions), len(transcript_text))

    matches = match_questions_in_transcript(transcript_text, questions)

    log.info("Matched %d of %d questions", len(matches), len(questions))

    # Insert matched Q&A pairs as content items
    items: list[dict] = []
    for match in matches:
        answer = match["answer"].strip()

        # Skip very short answers or explicit skips
        if len(answer) < MIN_ANSWER_CHARS:
            log.debug("Skipping Q%d (answer too short: %d chars)", match["q_number"], len(answer))
            continue

        item = {
            "source_file": str(transcript_path),
            "content_type": "interview",
            "title": match["question"],
            "text": answer,
            "author": author,
            "date": None,
            "metadata": {
                "question_id": match.get("question_id"),
                "question_number": match["q_number"],
                "category": match.get("category"),
                "framework": match.get("framework"),
            },
        }
        _insert_content_item(db, item)
        items.append(item)

    db.conn.commit()
    log.info("Inserted %d interview items", len(items))
    return items


def extract_questions_from_questionnaire(questionnaire_path: Path) -> list[dict]:
    """Parse questions from a generated questionnaire markdown file.

    Looks for patterns like: **Q1.** What's your name?
    """
    text = questionnaire_path.read_text()
    pattern = re.compile(
        r"\*\*Q(\d+)\.\*\*\s+(.+?)(?=\n\s*(?:\*\*Q\d+\.\*\*|##|---|\Z))",
        re.DOTALL,
    )

    questions = []
    for m in pattern.finditer(text):
        q_num = int(m.group(1))
        # The question text may include follow-up lines; take just the first line
        full_text = m.group(2).strip()
        first_line = full_text.split("\n")[0].strip()
        questions.append({
            "q_number": q_num,
            "question": first_line,
            "full_text": full_text,
        })

    return questions


def match_questions_in_transcript(
    transcript: str,
    questions: list[dict],
) -> list[dict]:
    """Match questions from the questionnaire against the transcript text.

    Uses multiple strategies:
    1. Look for "Q1" / "Question 1" markers
    2. Fuzzy-match the question text itself

    Returns list of matched Q&A pairs, sorted by position.
    """
    # Normalize transcript for matching
    transcript_lower = transcript.lower()

    # Build a list of (position, question_dict) tuples
    found: list[tuple[int, int, dict]] = []

    for q in questions:
        pos = _find_question_in_transcript(transcript, transcript_lower, q)
        if pos is not None:
            start_pos, end_of_question = pos
            found.append((start_pos, end_of_question, q))

    if not found:
        return []

    # Sort by position in transcript
    found.sort(key=lambda x: x[0])

    # Extract answers: text between end of question N and start of question N+1
    results = []
    for i, (start_pos, q_end, q) in enumerate(found):
        if i + 1 < len(found):
            answer_end = found[i + 1][0]
        else:
            answer_end = len(transcript)

        answer = transcript[q_end:answer_end].strip()

        results.append({
            "q_number": q["q_number"],
            "question": q["question"],
            "question_id": q.get("question_id"),
            "category": q.get("category"),
            "framework": q.get("framework"),
            "answer": answer,
            "start_pos": start_pos,
            "end_pos": answer_end,
        })

    return results


def _find_question_in_transcript(
    transcript: str,
    transcript_lower: str,
    question: dict,
) -> tuple[int, int] | None:
    """Find where a question appears in the transcript.

    Returns (start_of_question, end_of_question) positions, or None.
    """
    q_num = question["q_number"]
    q_text = question["question"]

    # Strategy 1: Look for explicit markers like "Q1" "Q 1" "question 1"
    marker_patterns = [
        rf"\bq\s*{q_num}\b",
        rf"\bquestion\s+{q_num}\b",
        rf"\bnumber\s+{q_num}\b",
    ]
    for pattern in marker_patterns:
        m = re.search(pattern, transcript_lower)
        if m:
            # Find where the answer starts (after the marker + any question text)
            marker_end = m.end()
            # Try to skip past the question text if it follows the marker
            remaining = transcript_lower[marker_end:marker_end + len(q_text) + 200]
            q_end = _skip_question_text(transcript, marker_end, q_text)
            return m.start(), q_end

    # Strategy 2: Fuzzy match the question text
    pos = _fuzzy_find(q_text.lower(), transcript_lower, threshold=0.65)
    if pos is not None:
        q_end = pos + len(q_text) + 50  # approximate end, allowing for transcription variance
        q_end = min(q_end, len(transcript))
        return pos, q_end

    return None


def _skip_question_text(
    transcript: str,
    after_marker: int,
    question_text: str,
) -> int:
    """After finding a marker (e.g. "Q5"), skip past any question text that follows.

    Returns the position where the answer likely begins.
    """
    # Look ahead up to question length + buffer
    window_size = len(question_text) + 200
    window = transcript[after_marker:after_marker + window_size]

    # Try to find a substantial portion of the question text
    q_words = question_text.lower().split()
    if len(q_words) >= 4:
        # Look for first 4 words as a sequence (fuzzy)
        snippet = " ".join(q_words[:4])
        idx = window.lower().find(snippet[:20])
        if idx >= 0:
            # Skip to the end of the question text
            q_end_approx = after_marker + idx + len(question_text)
            return min(q_end_approx, len(transcript))

    # If we can't find the question text, just skip a small buffer after the marker
    # Look for sentence boundary (period, newline)
    for i, ch in enumerate(window):
        if i > 20 and ch in ".?\n":
            return after_marker + i + 1

    return after_marker + min(50, len(window))


def _fuzzy_find(
    needle: str,
    haystack: str,
    threshold: float = 0.55,
    window_factor: float = 1.5,
) -> int | None:
    """Find approximate position of needle in haystack using sliding window.

    Returns starting position or None if no match above threshold.
    """
    needle_len = len(needle)
    if needle_len == 0 or len(haystack) == 0:
        return None

    window_size = int(needle_len * window_factor)
    best_ratio = 0.0
    best_pos = None

    # Use word-level stepping for performance (not char-by-char)
    step = max(1, needle_len // 6)

    for i in range(0, len(haystack) - needle_len + 1, step):
        window = haystack[i:i + window_size]
        ratio = difflib.SequenceMatcher(None, needle, window).ratio()
        if ratio > best_ratio:
            best_ratio = ratio
            best_pos = i

    if best_ratio >= threshold:
        return best_pos
    return None


def _insert_content_item(db: Database, item: dict) -> int:
    """Insert a single content item into the database."""
    text = item.get("text", "")
    metadata_json = json.dumps(item.get("metadata", {})) if item.get("metadata") else None

    cursor = db.conn.execute(
        """INSERT INTO content_items
           (source_file, content_type, title, text, author, date, metadata_json, char_count)
           VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
        (
            item.get("source_file"),
            item.get("content_type", "interview"),
            item.get("title"),
            text,
            item.get("author"),
            item.get("date"),
            metadata_json,
            len(text),
        ),
    )
    item["id"] = cursor.lastrowid
    return cursor.lastrowid
